Approximate computing research is still in its early stages.
This dissertation re-examines traditional abstractions in hardware and
software and argues that they should include a notion of computational
quality.
It develops five principles for the design of approximation-aware
abstractions:

\paragraph{Application-Specific Result Quality}
Applications from a broad spectrum of domains come with correctness constraints that are
not binary:
there are better outputs and worse outputs.
But as with traditional correctness criteria, there is no single, universal
``soft'' quality criterion.
A key principle in this work is that programmers should express
\emph{quality metrics} to quantify an output's usefulness on a continuous
scale.
Quality metrics are essential not only to the design of tools that constrain
correctness,
but also to the empirical evaluation of any approximation technique.

\paragraph{Safety vs.~Quality}
The abstractions in this dissertation benefit from decomposing correctness
into two complementary concerns:
\emph{quality}, the degree of accuracy for approximate values, and
\emph{safety}, whether to allow any degree of approximation at all.
While this zero-versus-nonzero distinction may at first seem artificial, it
decomposes many intractable problems into two smaller problems that can be
tractably solved using different tools.
EnerJ (\chref{enerj}) and DECAF (\chref{decaf}) demonstrate this separation of
concerns:
information flow types are best suited for safety,
and constraint-solving numerical type inference is best suited for quality.
Using a single technique for both would be less effective.

\paragraph{Hardwareâ€“Software Co-Design}
Approximation is a cross-cutting concern.
While both hardware and software techniques hold promise, a good rule of thumb
is to \emph{never do hardware without software}.
Hardware techniques that work opaquely---without incorporating any information
at all from the application---are easy to design but doomed to failure.
An approximate memory (\chref{approxstorage}) that can flip any bit with any
probability, for example,
ignores the software's complex needs for different levels of reliability for
different kinds of data.
Researchers should always design hardware techniques with the programming
abstraction in mind.

\paragraph{Programming with Probabilistic Reasoning}
Many of the best proposals for approximate-computing techniques are inherently
probabilistic:
an analog circuit~\cite{anpu} or a noisy memory write (\chref{approxstorage}),
for example, are nondeterministic by nature.
Even when approximation strategies themselves are deterministic, correctness
criteria can often be best expressed using probabilities: the chance that a
randomly selected input has high quality, or the chance that an individual
pixel in an image is wrong.
In both cases, approximation calls for programming languages to add constructs
reflecting probability and statistics.
\chref{decaf} develops a type-system approach to probabilistic reasoning,
and \chref{passert} explores a new way for programmers to express
general probabilistic bounds.

\paragraph{Granularity of Approximation}
Approximation techniques work by replacing some accurate part of a program
with a cheaper, less accurate counterpart.
A critical dimension in these techniques is the \emph{granularity} of
components they replace.
Approaches that replace individual arithmetic operations~\cite{truffle} can be
general and flexible, but their efficiency gains tend to be small.
Coarse-grained replacement techniques, such as neural acceleration~\cite{npu},
can be more complex to apply
but tend to offer larger gains.
The ACCEPT compiler framework in \chref{accept} represents a step toward
unifying an intelligible fine-grained programming abstraction
with powerful coarse-grained approximation strategies.

\vspace{\baselineskip}
\noindent
These principles should guide the next phase of research on new abstractions
for approximation.
